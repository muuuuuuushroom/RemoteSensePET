Running eval with robust_para=2.4
Not using distributed mode
Namespace(cfg='outputs/RS/Car/base_s4/config.yaml', gt_determined='100', device='cuda', resume='outputs/RS/Car/base_s4/best_checkpoint.pth', vis_dir='/data/zlt/RemoteSensePET/outputs/Ship/t_noencoder_attn_opre_bs16_withen_box1_layer6/vis', eval_pad='padding_center', eval_robust='scale', robust_para='2.4', prob_map_lc=None, world_size=1, dist_url='env://', Orattn=True, attn_splitter=True, attn_type='softmax', augmented=False, backbone='swin_t', backbone_num_channels=512, batch_size=8, box_setting=1, ce_loss_coef=1.0, clip_max_norm=0.1, context_patch=[64, 32], data_path='data/Car', dataset_file='Car', dec_layers=2, dense_dec_win_size=[4, 2], dense_stride=8, dim_feedforward=512, dist_backend='nccl', distributed=False, dropout=0.0, enc_win_list=[[16, 8], [16, 8], [8, 4], [8, 4]], encoder_free=True, eos_coef=0.5, epochs=1500, eval_freq=5, fpn_type='panet', global_crop_ratio=0.1, gpu=0, hidden_dim=256, loss_f='normal', lr=0.0001, lr_backbone=1e-05, map_loss_coef=1.0, matcher='hun', nheads=8, num_workers=2, one_key_hfy=True, one_key_zlt=False, opt_query_con=False, opt_query_decoder=False, output_dir='base_s4', patch_size=256, point_loss_coef=5.0, position_embedding='sine', predict='origin', prob_bandwidth='dynamic', rank=0, save_ckpt_freq=1000, seed=3407, set_cost_class=1, set_cost_point=0.05, sparse_dec_win_size=[8, 4], sparse_stride=16, start_epoch=0, syn_bn=0, upsample_strategy='dysample', use_arc=True, use_spatial_attention=True, weight_decay=0.0001)
build swin backbone
building PET without encoder
context_patch: [64, 32]
attn splitter
using detr object query for decoder
robust testing: scale
scaling test
static global crop ratio
load successfully from ckpt: outputs/RS/Car/base_s4/best_checkpoint.pth
Test:  [  0/228]  eta: 0:14:32  mae: 3.0000 (3.0000)  time: 3.8271  data: 0.5753  max mem: 14579
Traceback [1;36m(most recent call last)[0m:
[0m  File [0;32m/data/zlt/RemoteSensePET/eval_rebuild.py:143[0m
    main(args)[0m
[0m  File [0;32m/data/zlt/RemoteSensePET/eval_rebuild.py:103[0m in [0;35mmain[0m
    test_stats = evaluate(model, data_loader_val, device, vis_dir=vis_dir, args=args, criterion=criterion)[0m
[0m  File [0;32m~/ctf/anaconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115[0m in [0;35mdecorate_context[0m
    return func(*args, **kwargs)[0m
[0m  File [0;32m/data/zlt/RemoteSensePET/engine.py:306[0m in [0;35mevaluate[0m
    outputs = model(samples, test=True, targets=targets, criterion=criterion, eval_s=True)[0m
[0m  File [0;32m~/ctf/anaconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501[0m in [0;35m_call_impl[0m
    return forward_call(*args, **kwargs)[0m
[0m  File [0;32m/data/zlt/RemoteSensePET/models/pet.py:697[0m in [0;35mforward[0m
    dense_input_embed = self.pos_embed(samples)  # B*hidden_dim*imgH*imgW[0m
[0m  File [0;32m~/ctf/anaconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501[0m in [0;35m_call_impl[0m
    return forward_call(*args, **kwargs)[0m
[1;36m  File [1;32m/data/zlt/RemoteSensePET/models/position_encoding.py:49[1;36m in [1;35mforward[1;36m
[1;33m    pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)[1;36m
[1;31mOutOfMemoryError[0m[1;31m:[0m CUDA out of memory. Tried to allocate 5.62 GiB (GPU 0; 23.68 GiB total capacity; 6.27 GiB already allocated; 5.22 GiB free; 18.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Finished eval with robust_para=2.4
----------------------------------------
Running eval with robust_para=2.5
Not using distributed mode
Namespace(cfg='outputs/RS/Car/base_s4/config.yaml', gt_determined='100', device='cuda', resume='outputs/RS/Car/base_s4/best_checkpoint.pth', vis_dir='/data/zlt/RemoteSensePET/outputs/Ship/t_noencoder_attn_opre_bs16_withen_box1_layer6/vis', eval_pad='padding_center', eval_robust='scale', robust_para='2.5', prob_map_lc=None, world_size=1, dist_url='env://', Orattn=True, attn_splitter=True, attn_type='softmax', augmented=False, backbone='swin_t', backbone_num_channels=512, batch_size=8, box_setting=1, ce_loss_coef=1.0, clip_max_norm=0.1, context_patch=[64, 32], data_path='data/Car', dataset_file='Car', dec_layers=2, dense_dec_win_size=[4, 2], dense_stride=8, dim_feedforward=512, dist_backend='nccl', distributed=False, dropout=0.0, enc_win_list=[[16, 8], [16, 8], [8, 4], [8, 4]], encoder_free=True, eos_coef=0.5, epochs=1500, eval_freq=5, fpn_type='panet', global_crop_ratio=0.1, gpu=0, hidden_dim=256, loss_f='normal', lr=0.0001, lr_backbone=1e-05, map_loss_coef=1.0, matcher='hun', nheads=8, num_workers=2, one_key_hfy=True, one_key_zlt=False, opt_query_con=False, opt_query_decoder=False, output_dir='base_s4', patch_size=256, point_loss_coef=5.0, position_embedding='sine', predict='origin', prob_bandwidth='dynamic', rank=0, save_ckpt_freq=1000, seed=3407, set_cost_class=1, set_cost_point=0.05, sparse_dec_win_size=[8, 4], sparse_stride=16, start_epoch=0, syn_bn=0, upsample_strategy='dysample', use_arc=True, use_spatial_attention=True, weight_decay=0.0001)
build swin backbone
building PET without encoder
context_patch: [64, 32]
attn splitter
using detr object query for decoder
robust testing: scale
scaling test
static global crop ratio
load successfully from ckpt: outputs/RS/Car/base_s4/best_checkpoint.pth
Running eval with robust_para=0.2
